{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiPWmRVAl4VN"
      },
      "source": [
        "Mount dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehfEoDjmReFv",
        "outputId": "409647cc-3e7b-4644-dcf2-9f5052e889f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UgDxfsuiY1R"
      },
      "outputs": [],
      "source": [
        "!cp -r drive/MyDrive/ColabNotebooks/cnnPred/Dataset/ /content/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlDN2CQxmPVK"
      },
      "source": [
        "# **Import libraries:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FljJZAlmFpU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import scaledata\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from os.path import join\n",
        "from os import listdir\n",
        "from sklearn.metrics import accuracy_score, f1_score, mean_absolute_error\n",
        "# import os\n",
        "import random\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, Input\n",
        "# from pathlib2 import Path\n",
        "from tensorflow.keras import backend as K, callbacks\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import MeanSquaredLogarithmicError, MSE, MAE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWPe0F84mVwZ"
      },
      "source": [
        "# **Declare constant:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxHO8gyJmYeN"
      },
      "outputs": [],
      "source": [
        "DATADIR = \"/content/Dataset\"\n",
        "TRAIN_TEST_CUTOFF = '2016-04-21'\n",
        "TRAIN_VALID_RATIO = 0.75\n",
        "\n",
        "seq_len = 60\n",
        "batch_size = 128\n",
        "n_epochs = 20\n",
        "n_features = 82"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCZB9B-KmkFB"
      },
      "source": [
        "# **Data Preprocessing:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZksQlICQmyfB",
        "outputId": "0f53b350-7ebb-403b-8f42-7e3af1174f69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data we have: \n",
            "NYA | S&P | RUT | NASDAQ | DJI | "
          ]
        }
      ],
      "source": [
        "from os.path import join\n",
        "from os import listdir\n",
        "\n",
        "\n",
        "data = {}\n",
        "\n",
        "print(\"data we have: \")\n",
        "for filename in listdir(DATADIR):\n",
        "    if not filename.lower().endswith(\".csv\"):\n",
        "        continue # read only the CSV files\n",
        "\n",
        "    filepath = join(DATADIR, filename)\n",
        "    X = pd.read_csv(filepath, index_col=\"Date\", parse_dates=True)\n",
        "    # basic preprocessing: get the name, the classification\n",
        "    # Save the target variable as a column in dataframe for easier dropna()\n",
        "    name = X[\"Name\"][0]\n",
        "    print(X[\"Name\"][0], end=' | ')\n",
        "    del X[\"Name\"]\n",
        "    cols = X.columns\n",
        "    # The line of code above is to compute the percentage change of the closing \n",
        "    # index and align the data with the previous day. Then convert the data into \n",
        "    # either 1 or 0 for whether the percentage change is positive.\n",
        "    X[\"Target\"] = (X[\"Close\"].pct_change().shift(-1) > 0).astype(int) \n",
        "    X.dropna(inplace=True)\n",
        "    # Fit the standard scaler using the training dataset\n",
        "    index = X.index[X.index > TRAIN_TEST_CUTOFF]\n",
        "    index = index[:int(len(index) * TRAIN_VALID_RATIO)]\n",
        "    scaler = StandardScaler().fit(X.loc[index, cols])\n",
        "    # Save scale transformed dataframe\n",
        "    X[cols] = scaler.transform(X[cols])\n",
        "    data[name] = X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 710
        },
        "id": "7x6ZnCWam24X",
        "outputId": "e8461602-776b-4f74-e93d-532edf4ef14d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-d7b9345b-6205-48a3-8629-52f7ac020b64\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>mom</th>\n",
              "      <th>mom1</th>\n",
              "      <th>mom2</th>\n",
              "      <th>mom3</th>\n",
              "      <th>ROC_5</th>\n",
              "      <th>ROC_10</th>\n",
              "      <th>ROC_15</th>\n",
              "      <th>ROC_20</th>\n",
              "      <th>...</th>\n",
              "      <th>silver-F</th>\n",
              "      <th>RUSSELL-F</th>\n",
              "      <th>S&amp;P-F</th>\n",
              "      <th>CHF</th>\n",
              "      <th>Dollar index-F</th>\n",
              "      <th>Dollar index</th>\n",
              "      <th>wheat-F</th>\n",
              "      <th>XAG</th>\n",
              "      <th>XAU</th>\n",
              "      <th>Target</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2010-10-19</th>\n",
              "      <td>-8.770014</td>\n",
              "      <td>1.232144</td>\n",
              "      <td>-3.521385</td>\n",
              "      <td>0.921290</td>\n",
              "      <td>-0.543432</td>\n",
              "      <td>-0.366935</td>\n",
              "      <td>-0.948677</td>\n",
              "      <td>-0.511275</td>\n",
              "      <td>0.260829</td>\n",
              "      <td>0.537023</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.687757</td>\n",
              "      <td>-1.957717</td>\n",
              "      <td>-2.512805</td>\n",
              "      <td>2.946985</td>\n",
              "      <td>4.084704</td>\n",
              "      <td>4.087215</td>\n",
              "      <td>-1.723464</td>\n",
              "      <td>-3.030445</td>\n",
              "      <td>-3.179746</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-10-20</th>\n",
              "      <td>-8.518337</td>\n",
              "      <td>-0.586797</td>\n",
              "      <td>2.083192</td>\n",
              "      <td>-3.308400</td>\n",
              "      <td>0.852650</td>\n",
              "      <td>-0.585733</td>\n",
              "      <td>-0.665580</td>\n",
              "      <td>0.158863</td>\n",
              "      <td>1.015944</td>\n",
              "      <td>1.432813</td>\n",
              "      <td>...</td>\n",
              "      <td>0.690639</td>\n",
              "      <td>0.791223</td>\n",
              "      <td>1.511489</td>\n",
              "      <td>-2.225994</td>\n",
              "      <td>-3.141538</td>\n",
              "      <td>-3.203745</td>\n",
              "      <td>1.762781</td>\n",
              "      <td>1.802923</td>\n",
              "      <td>1.136816</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-10-21</th>\n",
              "      <td>-8.538791</td>\n",
              "      <td>-0.475087</td>\n",
              "      <td>-0.394430</td>\n",
              "      <td>2.015788</td>\n",
              "      <td>-2.746574</td>\n",
              "      <td>0.924213</td>\n",
              "      <td>-0.599892</td>\n",
              "      <td>0.279328</td>\n",
              "      <td>1.088429</td>\n",
              "      <td>1.859468</td>\n",
              "      <td>...</td>\n",
              "      <td>-2.410474</td>\n",
              "      <td>-0.702541</td>\n",
              "      <td>-0.064847</td>\n",
              "      <td>1.517609</td>\n",
              "      <td>0.704292</td>\n",
              "      <td>0.830085</td>\n",
              "      <td>-2.072089</td>\n",
              "      <td>-2.384037</td>\n",
              "      <td>-1.935332</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-10-26</th>\n",
              "      <td>-8.500773</td>\n",
              "      <td>-0.092545</td>\n",
              "      <td>-0.561486</td>\n",
              "      <td>0.340683</td>\n",
              "      <td>0.062240</td>\n",
              "      <td>-0.234660</td>\n",
              "      <td>0.772057</td>\n",
              "      <td>-0.110089</td>\n",
              "      <td>0.136768</td>\n",
              "      <td>0.804475</td>\n",
              "      <td>...</td>\n",
              "      <td>1.603891</td>\n",
              "      <td>-0.391340</td>\n",
              "      <td>-0.231754</td>\n",
              "      <td>2.720100</td>\n",
              "      <td>1.953575</td>\n",
              "      <td>2.007663</td>\n",
              "      <td>2.535224</td>\n",
              "      <td>0.634982</td>\n",
              "      <td>0.022028</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-10-27</th>\n",
              "      <td>-8.626234</td>\n",
              "      <td>0.086975</td>\n",
              "      <td>-1.337634</td>\n",
              "      <td>-0.496580</td>\n",
              "      <td>0.358587</td>\n",
              "      <td>0.069337</td>\n",
              "      <td>-0.719090</td>\n",
              "      <td>-1.047763</td>\n",
              "      <td>-0.290349</td>\n",
              "      <td>0.553612</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.937423</td>\n",
              "      <td>-0.422460</td>\n",
              "      <td>-0.862288</td>\n",
              "      <td>1.131904</td>\n",
              "      <td>1.488156</td>\n",
              "      <td>1.456456</td>\n",
              "      <td>0.457148</td>\n",
              "      <td>-0.885546</td>\n",
              "      <td>-1.339050</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-08-09</th>\n",
              "      <td>2.551948</td>\n",
              "      <td>-0.126560</td>\n",
              "      <td>-0.502278</td>\n",
              "      <td>-0.672391</td>\n",
              "      <td>-0.037127</td>\n",
              "      <td>0.278825</td>\n",
              "      <td>-0.605010</td>\n",
              "      <td>-0.600994</td>\n",
              "      <td>-0.555871</td>\n",
              "      <td>-0.211174</td>\n",
              "      <td>...</td>\n",
              "      <td>1.919258</td>\n",
              "      <td>-1.044862</td>\n",
              "      <td>-0.213208</td>\n",
              "      <td>-2.430191</td>\n",
              "      <td>-0.226546</td>\n",
              "      <td>-0.247273</td>\n",
              "      <td>0.450312</td>\n",
              "      <td>1.949834</td>\n",
              "      <td>1.499770</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-08-10</th>\n",
              "      <td>2.155284</td>\n",
              "      <td>0.405858</td>\n",
              "      <td>-2.460125</td>\n",
              "      <td>-0.440334</td>\n",
              "      <td>-0.503481</td>\n",
              "      <td>-0.038135</td>\n",
              "      <td>-1.441596</td>\n",
              "      <td>-1.358462</td>\n",
              "      <td>-1.223169</td>\n",
              "      <td>-0.919267</td>\n",
              "      <td>...</td>\n",
              "      <td>0.802332</td>\n",
              "      <td>-2.196304</td>\n",
              "      <td>-2.902253</td>\n",
              "      <td>-0.184029</td>\n",
              "      <td>-0.324528</td>\n",
              "      <td>-0.372548</td>\n",
              "      <td>-2.858203</td>\n",
              "      <td>0.899421</td>\n",
              "      <td>1.162741</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-08-16</th>\n",
              "      <td>2.399676</td>\n",
              "      <td>-0.001186</td>\n",
              "      <td>0.153807</td>\n",
              "      <td>-0.334502</td>\n",
              "      <td>1.014468</td>\n",
              "      <td>-0.179659</td>\n",
              "      <td>-0.672641</td>\n",
              "      <td>-0.964319</td>\n",
              "      <td>-0.904022</td>\n",
              "      <td>-0.914748</td>\n",
              "      <td>...</td>\n",
              "      <td>0.808902</td>\n",
              "      <td>-0.080139</td>\n",
              "      <td>0.046423</td>\n",
              "      <td>-1.522651</td>\n",
              "      <td>-0.814443</td>\n",
              "      <td>-0.798480</td>\n",
              "      <td>-1.668778</td>\n",
              "      <td>1.905761</td>\n",
              "      <td>1.097928</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-08-17</th>\n",
              "      <td>2.007334</td>\n",
              "      <td>0.251304</td>\n",
              "      <td>-2.446978</td>\n",
              "      <td>0.182927</td>\n",
              "      <td>-0.215957</td>\n",
              "      <td>1.099229</td>\n",
              "      <td>-0.666869</td>\n",
              "      <td>-1.612134</td>\n",
              "      <td>-1.543241</td>\n",
              "      <td>-1.547507</td>\n",
              "      <td>...</td>\n",
              "      <td>0.454114</td>\n",
              "      <td>-2.227424</td>\n",
              "      <td>-3.087704</td>\n",
              "      <td>-0.637799</td>\n",
              "      <td>0.263369</td>\n",
              "      <td>0.253824</td>\n",
              "      <td>-0.814306</td>\n",
              "      <td>-0.209756</td>\n",
              "      <td>0.631273</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-08-22</th>\n",
              "      <td>2.239940</td>\n",
              "      <td>-0.090680</td>\n",
              "      <td>1.037340</td>\n",
              "      <td>0.105178</td>\n",
              "      <td>-0.221385</td>\n",
              "      <td>-2.029208</td>\n",
              "      <td>-0.535287</td>\n",
              "      <td>-1.131537</td>\n",
              "      <td>-1.309946</td>\n",
              "      <td>-1.262952</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.110920</td>\n",
              "      <td>1.029810</td>\n",
              "      <td>1.659850</td>\n",
              "      <td>1.585674</td>\n",
              "      <td>1.194207</td>\n",
              "      <td>1.256018</td>\n",
              "      <td>-1.128752</td>\n",
              "      <td>-0.268520</td>\n",
              "      <td>-0.768694</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1114 rows Ã— 83 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d7b9345b-6205-48a3-8629-52f7ac020b64')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d7b9345b-6205-48a3-8629-52f7ac020b64 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d7b9345b-6205-48a3-8629-52f7ac020b64');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "               Close    Volume       mom      mom1      mom2      mom3  \\\n",
              "Date                                                                     \n",
              "2010-10-19 -8.770014  1.232144 -3.521385  0.921290 -0.543432 -0.366935   \n",
              "2010-10-20 -8.518337 -0.586797  2.083192 -3.308400  0.852650 -0.585733   \n",
              "2010-10-21 -8.538791 -0.475087 -0.394430  2.015788 -2.746574  0.924213   \n",
              "2010-10-26 -8.500773 -0.092545 -0.561486  0.340683  0.062240 -0.234660   \n",
              "2010-10-27 -8.626234  0.086975 -1.337634 -0.496580  0.358587  0.069337   \n",
              "...              ...       ...       ...       ...       ...       ...   \n",
              "2017-08-09  2.551948 -0.126560 -0.502278 -0.672391 -0.037127  0.278825   \n",
              "2017-08-10  2.155284  0.405858 -2.460125 -0.440334 -0.503481 -0.038135   \n",
              "2017-08-16  2.399676 -0.001186  0.153807 -0.334502  1.014468 -0.179659   \n",
              "2017-08-17  2.007334  0.251304 -2.446978  0.182927 -0.215957  1.099229   \n",
              "2017-08-22  2.239940 -0.090680  1.037340  0.105178 -0.221385 -2.029208   \n",
              "\n",
              "               ROC_5    ROC_10    ROC_15    ROC_20  ...  silver-F  RUSSELL-F  \\\n",
              "Date                                                ...                        \n",
              "2010-10-19 -0.948677 -0.511275  0.260829  0.537023  ... -1.687757  -1.957717   \n",
              "2010-10-20 -0.665580  0.158863  1.015944  1.432813  ...  0.690639   0.791223   \n",
              "2010-10-21 -0.599892  0.279328  1.088429  1.859468  ... -2.410474  -0.702541   \n",
              "2010-10-26  0.772057 -0.110089  0.136768  0.804475  ...  1.603891  -0.391340   \n",
              "2010-10-27 -0.719090 -1.047763 -0.290349  0.553612  ... -1.937423  -0.422460   \n",
              "...              ...       ...       ...       ...  ...       ...        ...   \n",
              "2017-08-09 -0.605010 -0.600994 -0.555871 -0.211174  ...  1.919258  -1.044862   \n",
              "2017-08-10 -1.441596 -1.358462 -1.223169 -0.919267  ...  0.802332  -2.196304   \n",
              "2017-08-16 -0.672641 -0.964319 -0.904022 -0.914748  ...  0.808902  -0.080139   \n",
              "2017-08-17 -0.666869 -1.612134 -1.543241 -1.547507  ...  0.454114  -2.227424   \n",
              "2017-08-22 -0.535287 -1.131537 -1.309946 -1.262952  ... -0.110920   1.029810   \n",
              "\n",
              "               S&P-F       CHF  Dollar index-F  Dollar index   wheat-F  \\\n",
              "Date                                                                     \n",
              "2010-10-19 -2.512805  2.946985        4.084704      4.087215 -1.723464   \n",
              "2010-10-20  1.511489 -2.225994       -3.141538     -3.203745  1.762781   \n",
              "2010-10-21 -0.064847  1.517609        0.704292      0.830085 -2.072089   \n",
              "2010-10-26 -0.231754  2.720100        1.953575      2.007663  2.535224   \n",
              "2010-10-27 -0.862288  1.131904        1.488156      1.456456  0.457148   \n",
              "...              ...       ...             ...           ...       ...   \n",
              "2017-08-09 -0.213208 -2.430191       -0.226546     -0.247273  0.450312   \n",
              "2017-08-10 -2.902253 -0.184029       -0.324528     -0.372548 -2.858203   \n",
              "2017-08-16  0.046423 -1.522651       -0.814443     -0.798480 -1.668778   \n",
              "2017-08-17 -3.087704 -0.637799        0.263369      0.253824 -0.814306   \n",
              "2017-08-22  1.659850  1.585674        1.194207      1.256018 -1.128752   \n",
              "\n",
              "                 XAG       XAU  Target  \n",
              "Date                                    \n",
              "2010-10-19 -3.030445 -3.179746       1  \n",
              "2010-10-20  1.802923  1.136816       0  \n",
              "2010-10-21 -2.384037 -1.935332       1  \n",
              "2010-10-26  0.634982  0.022028       0  \n",
              "2010-10-27 -0.885546 -1.339050       1  \n",
              "...              ...       ...     ...  \n",
              "2017-08-09  1.949834  1.499770       0  \n",
              "2017-08-10  0.899421  1.162741       0  \n",
              "2017-08-16  1.905761  1.097928       0  \n",
              "2017-08-17 -0.209756  0.631273       0  \n",
              "2017-08-22 -0.268520 -0.768694       0  \n",
              "\n",
              "[1114 rows x 83 columns]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data['NYA']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CG4i5IHyrO3w"
      },
      "source": [
        "# **Trainset genarator:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhbQkuc8m3YT"
      },
      "outputs": [],
      "source": [
        "def datagen(data, seq_len, batch_size, targetcol, kind):\n",
        "    \"As a generator to produce samples for Keras model\"\n",
        "    batch = []\n",
        "    while True:\n",
        "        # Pick one dataframe from the pool\n",
        "        key = random.choice(list(data.keys()))\n",
        "        df = data[key]\n",
        "        input_cols = [c for c in df.columns if c != targetcol]\n",
        "        index = df.index[df.index < TRAIN_TEST_CUTOFF]\n",
        "        split = int(len(index) * TRAIN_VALID_RATIO)\n",
        "        assert split > seq_len, \"Training data too small for sequence length {}\".format(seq_len)\n",
        "        if kind == 'train':\n",
        "            index = index[:split]   # range for the training set\n",
        "        elif kind == 'valid':\n",
        "            index = index[split:]   # range for the validation set\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "        # Pick one position, then clip a sequence length\n",
        "        while True:\n",
        "            t = random.choice(index)     # pick one time step\n",
        "            n = (df.index == t).argmax() # find its position in the dataframe\n",
        "            if n-seq_len+1 < 0:\n",
        "                continue # this sample is not enough for one sequence length\n",
        "            frame = df.iloc[n-seq_len+1:n+1]\n",
        "            batch.append([frame[input_cols].values, df.loc[t, targetcol]])\n",
        "            break\n",
        "        # if we get enough for a batch, dispatch\n",
        "        if len(batch) == batch_size:\n",
        "            X, y = zip(*batch)\n",
        "            X, y = np.expand_dims(np.array(X), 3), np.array(y)\n",
        "            yield X, y\n",
        "            batch = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fygWQXSBrCs5"
      },
      "source": [
        "# **Metrics:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNdfBqm6rFkW"
      },
      "outputs": [],
      "source": [
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        " \n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        " \n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        " \n",
        "def f1macro(y_true, y_pred):\n",
        "    f_pos = f1_m(y_true, y_pred)\n",
        "    # negative version of the data and prediction\n",
        "    f_neg = f1_m(1-y_true, 1-K.clip(y_pred,0,1))\n",
        "    return (f_pos + f_neg)/2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc8QwmWQ1a4M"
      },
      "source": [
        "# **Define Model:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEhf36Yiraap"
      },
      "outputs": [],
      "source": [
        "def cnnpred_2d(seq_len=60, n_features=82, n_filters=(8,8,8), droprate=0.1):\n",
        "    \"2D-CNNpred model according to the paper\"\n",
        "    model = Sequential([\n",
        "        Input(shape=(seq_len, n_features, 1)),\n",
        "        Conv2D(n_filters[0], kernel_size=(1, n_features), activation=\"relu\"),\n",
        "        Conv2D(n_filters[1], kernel_size=(3,1), activation=\"relu\"),\n",
        "        MaxPool2D(pool_size=(2,1)),\n",
        "        Conv2D(n_filters[2], kernel_size=(3,1), activation=\"relu\"),\n",
        "        MaxPool2D(pool_size=(2,1)),\n",
        "        Flatten(),\n",
        "        Dropout(droprate),\n",
        "        Dense(1, activation=\"sigmoid\")\n",
        "    ])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FW1-0atE1YiO"
      },
      "outputs": [],
      "source": [
        "def cnnpred_2d_mine(seq_len=60, n_features=82, n_filters=(8,8,8), droprate=0.1):\n",
        "    \"2D-CNNpred My own model architecture\"\n",
        "    model = Sequential([\n",
        "        Input(shape=(seq_len, n_features, 1)),\n",
        "        Conv2D(n_filters[0], kernel_size=(1, n_features), activation=\"relu\"),\n",
        "        Conv2D(n_filters[1], kernel_size=(3,1), activation=\"relu\"),\n",
        "        MaxPool2D(pool_size=(2,1)),\n",
        "        Conv2D(n_filters[2], kernel_size=(3,1), activation=\"relu\"),\n",
        "        MaxPool2D(pool_size=(2,1)),\n",
        "        Flatten(),\n",
        "        Dropout(droprate),\n",
        "        Dense(1, activation=\"sigmoid\")\n",
        "    ])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2UxvKnoQ52d"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, GlobalMaxPooling2D, BatchNormalization, Activation\n",
        "from tensorflow.keras.constraints import MaxNorm\n",
        "from tensorflow.keras.regularizers import Regularizer, L2\n",
        "\n",
        "def cnnpred_2d_resnet(seq_len=60, n_features=82, n_filters=(8,8,8), droprate=0.1):\n",
        "    model = Sequential()\n",
        "    model.add(ResNet50(include_top=False, weights=None, input_shape=(seq_len, n_features, 1)))\n",
        "    model.add(GlobalMaxPooling2D())\n",
        "    model.add(Dense(512, activation='relu', kernel_regularizer=L2(l2=0.015), kernel_constraint=MaxNorm(2)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dense(64, activation='relu', kernel_regularizer=L2(l2=0.01), kernel_constraint=MaxNorm(2)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZX1no8m-1s1V"
      },
      "source": [
        "# **Fit and Train**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfjhZWI172rc",
        "outputId": "b3f6f66c-dc67-4430-b2dc-aa547b52801f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_6 (Conv2D)           (None, 60, 1, 8)          664       \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 58, 1, 8)          200       \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPooling  (None, 29, 1, 8)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 27, 1, 8)          200       \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPooling  (None, 13, 1, 8)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 104)               0         \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 104)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 105       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,169\n",
            "Trainable params: 1,169\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Produce CNNpred as a binary classification problem\n",
        "model = cnnpred_2d_mine(seq_len, n_features)\n",
        "\n",
        "# loss = MSE(\n",
        "#     reduction=\"auto\", name=\"mean_squared_logarithmic_error\"\n",
        "# )\n",
        "opt = Adam(learning_rate=0.0001)\n",
        "model.compile(optimizer=opt, loss='mae', metrics=[\"acc\", f1macro])\n",
        "model.summary()  # print model structure to console"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--2ySFH0RRYd",
        "outputId": "fa3d7448-85a1-4185-9b1f-e2ca21ed5de9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "400/400 [==============================] - 64s 155ms/step - loss: 0.4585 - acc: 0.5501 - f1macro: 0.3855 - val_loss: 0.4978 - val_acc: 0.5031 - val_f1macro: 0.3343\n",
            "Epoch 2/20\n",
            "400/400 [==============================] - 62s 156ms/step - loss: 0.4446 - acc: 0.5562 - f1macro: 0.3568 - val_loss: 0.4755 - val_acc: 0.5281 - val_f1macro: 0.3454\n",
            "Epoch 3/20\n",
            "400/400 [==============================] - 62s 155ms/step - loss: 0.4479 - acc: 0.5524 - f1macro: 0.3553 - val_loss: 0.4760 - val_acc: 0.5266 - val_f1macro: 0.3447\n",
            "Epoch 4/20\n",
            "400/400 [==============================] - 63s 159ms/step - loss: 0.4355 - acc: 0.5647 - f1macro: 0.3604 - val_loss: 0.4892 - val_acc: 0.5117 - val_f1macro: 0.3380\n",
            "Epoch 5/20\n",
            "400/400 [==============================] - 61s 152ms/step - loss: 0.4381 - acc: 0.5620 - f1macro: 0.3593 - val_loss: 0.4895 - val_acc: 0.5117 - val_f1macro: 0.3381\n",
            "Epoch 6/20\n",
            "400/400 [==============================] - 62s 154ms/step - loss: 0.4389 - acc: 0.5611 - f1macro: 0.3589 - val_loss: 0.5093 - val_acc: 0.4914 - val_f1macro: 0.3289\n",
            "Epoch 7/20\n",
            "400/400 [==============================] - 61s 152ms/step - loss: 0.4391 - acc: 0.5609 - f1macro: 0.3589 - val_loss: 0.4855 - val_acc: 0.5148 - val_f1macro: 0.3393\n",
            "Epoch 8/20\n",
            "400/400 [==============================] - 61s 153ms/step - loss: 0.4361 - acc: 0.5639 - f1macro: 0.3601 - val_loss: 0.4659 - val_acc: 0.5344 - val_f1macro: 0.3479\n",
            "Epoch 9/20\n",
            "400/400 [==============================] - 60s 150ms/step - loss: 0.4418 - acc: 0.5582 - f1macro: 0.3577 - val_loss: 0.4733 - val_acc: 0.5273 - val_f1macro: 0.3442\n",
            "Epoch 10/20\n",
            "400/400 [==============================] - 61s 152ms/step - loss: 0.4413 - acc: 0.5587 - f1macro: 0.3580 - val_loss: 0.4745 - val_acc: 0.5258 - val_f1macro: 0.3441\n",
            "Epoch 11/20\n",
            "400/400 [==============================] - 59s 148ms/step - loss: 0.4377 - acc: 0.5623 - f1macro: 0.3594 - val_loss: 0.4979 - val_acc: 0.5023 - val_f1macro: 0.3335\n",
            "Epoch 12/20\n",
            "400/400 [==============================] - 61s 152ms/step - loss: 0.4399 - acc: 0.5601 - f1macro: 0.3585 - val_loss: 0.4994 - val_acc: 0.5008 - val_f1macro: 0.3332\n",
            "Epoch 13/20\n",
            "400/400 [==============================] - 60s 151ms/step - loss: 0.4427 - acc: 0.5573 - f1macro: 0.3574 - val_loss: 0.4947 - val_acc: 0.5055 - val_f1macro: 0.3352\n",
            "Epoch 14/20\n",
            "400/400 [==============================] - 61s 152ms/step - loss: 0.4414 - acc: 0.5586 - f1macro: 0.3579 - val_loss: 0.4799 - val_acc: 0.5203 - val_f1macro: 0.3415\n",
            "Epoch 15/20\n",
            "400/400 [==============================] - 59s 148ms/step - loss: 0.4358 - acc: 0.5642 - f1macro: 0.3602 - val_loss: 0.4954 - val_acc: 0.5047 - val_f1macro: 0.3350\n",
            "Epoch 16/20\n",
            "400/400 [==============================] - 61s 152ms/step - loss: 0.4404 - acc: 0.5596 - f1macro: 0.3583 - val_loss: 0.4511 - val_acc: 0.5492 - val_f1macro: 0.3541\n",
            "Epoch 17/20\n",
            "400/400 [==============================] - 59s 149ms/step - loss: 0.4380 - acc: 0.5620 - f1macro: 0.3593 - val_loss: 0.4604 - val_acc: 0.5398 - val_f1macro: 0.3496\n",
            "Epoch 18/20\n",
            "400/400 [==============================] - 62s 155ms/step - loss: 0.4383 - acc: 0.5617 - f1macro: 0.3592 - val_loss: 0.4916 - val_acc: 0.5086 - val_f1macro: 0.3369\n",
            "Epoch 19/20\n",
            "400/400 [==============================] - 63s 158ms/step - loss: 0.4424 - acc: 0.5576 - f1macro: 0.3575 - val_loss: 0.4628 - val_acc: 0.5375 - val_f1macro: 0.3492\n",
            "Epoch 20/20\n",
            "400/400 [==============================] - 64s 160ms/step - loss: 0.4405 - acc: 0.5595 - f1macro: 0.3583 - val_loss: 0.4847 - val_acc: 0.5156 - val_f1macro: 0.3398\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9f60890b90>"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Set up callbacks and fit the model\n",
        "# We use custom validation score f1macro() and hence monitor for \"val_f1macro\"\n",
        "checkpoint_path = \"./cp2d-{epoch}-{val_f1macro:.2f}.h5\"\n",
        "callbacks = [\n",
        "    ModelCheckpoint(checkpoint_path,\n",
        "                    monitor='val_f1macro', mode=\"max\",\n",
        "                    verbose=0, save_best_only=True, save_weights_only=False, save_freq=\"epoch\")\n",
        "]\n",
        "model.fit(datagen(data, seq_len, batch_size, \"Target\", \"train\"),\n",
        "          validation_data=datagen(data, seq_len, batch_size, \"Target\", \"valid\"),\n",
        "          epochs=n_epochs, steps_per_epoch=400, validation_steps=10, verbose=1, callbacks=callbacks)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2LC8T_drpBf"
      },
      "source": [
        "# **Test:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3nK1hH8rrmG"
      },
      "outputs": [],
      "source": [
        "def testgen(data, seq_len, targetcol):\n",
        "    \"Return array of all test samples\"\n",
        "    batch = []\n",
        "    for key, df in data.items():\n",
        "        input_cols = [c for c in df.columns if c != targetcol]\n",
        "        # find the start of test sample\n",
        "        t = df.index[df.index >= TRAIN_TEST_CUTOFF][0]\n",
        "        n = (df.index == t).argmax()\n",
        "        for i in range(n+1, len(df)+1):\n",
        "            frame = df.iloc[i-seq_len:i]\n",
        "            batch.append([frame[input_cols].values, frame[targetcol][-1]])\n",
        "    X, y = zip(*batch)\n",
        "    return np.expand_dims(np.array(X),3), np.array(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fg6uvu7Jr3wq",
        "outputId": "ca6f6b4e-2faa-424e-8226-f6b76228008f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy: 0.5434146341463415\n",
            "MAE: 0.45658536585365855\n",
            "F1: 0.7\n"
          ]
        }
      ],
      "source": [
        "# Prepare test data\n",
        "test_data, test_target = testgen(data, seq_len, \"Target\")\n",
        " \n",
        "# Test the model\n",
        "test_out = model.predict(test_data)\n",
        "test_pred = (test_out > 0.5).astype(int)\n",
        "print(\"accuracy:\", accuracy_score(test_pred, test_target))\n",
        "print(\"MAE:\", mean_absolute_error(test_pred, test_target))\n",
        "print(\"F1:\", f1_score(test_pred, test_target))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAVrPPg6g4hV"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "CNNPred_2D.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
